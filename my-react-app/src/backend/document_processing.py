"""
document_processing.py - æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–æ¨¡å—

è¿™æ˜¯RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
1. ğŸ“„ å¤šæ ¼å¼æ–‡æ¡£åŠ è½½ - æ”¯æŒTXTã€PDFã€DOCXæ–‡ä»¶
2. ğŸ”¤ æ™ºèƒ½æ–‡æœ¬åˆ†å‰² - é€’å½’å­—ç¬¦åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
3. ğŸ§® å‘é‡åŒ–å­˜å‚¨ - ä½¿ç”¨ChromaDBæ„å»ºå‘é‡æ•°æ®åº“
4. ğŸ” ç›¸ä¼¼æ€§æ£€ç´¢ - åŸºäºåµŒå…¥å‘é‡çš„æ–‡æ¡£æ£€ç´¢
5. ğŸ—‘ï¸ èµ„æºç®¡ç† - è‡ªåŠ¨å†…å­˜ç®¡ç†å’Œå®ä¾‹æ¸…ç†
6. ğŸŒ å¤šç¼–ç æ”¯æŒ - æ™ºèƒ½æ£€æµ‹å’Œå¤„ç†å¤šç§æ–‡æœ¬ç¼–ç 

æŠ€æœ¯æ ˆ:
- LangChain: æ–‡æ¡£åŠ è½½å’Œå¤„ç†æ¡†æ¶
- ChromaDB: å‘é‡æ•°æ®åº“
- Ollama Embeddings: æœ¬åœ°åµŒå…¥æ¨¡å‹
- RecursiveCharacterTextSplitter: æ™ºèƒ½æ–‡æœ¬åˆ†å‰²

è®¾è®¡ç‰¹è‰²:
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„ç»Ÿä¸€å¤„ç†
- æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼Œå…¼å®¹ä¸­æ–‡ç¯å¢ƒ
- èµ„æºç”Ÿå‘½å‘¨æœŸè‡ªåŠ¨ç®¡ç†
- é”™è¯¯æ¢å¤å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶
"""

import os
import shutil
import time
import gc
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings  # æ›´æ–°åçš„å¯¼å…¥æ–¹å¼
import logging

# ========================= ç¯å¢ƒé…ç½® =========================

# ç¦ç”¨ ChromaDB çš„é¥æµ‹åŠŸèƒ½ï¼Œä¿æŠ¤ç”¨æˆ·éšç§
os.environ["ANONYMIZED_TELEMETRY"] = "False"

# è®¾ç½®Chromaæ•°æ®åº“åŸºç¡€è·¯å¾„
CHROMA_BASE_PATH = "chroma_db"
# ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
os.makedirs(CHROMA_BASE_PATH, exist_ok=True)

def get_user_chroma_path(user_id: str) -> str:
    """
    è·å–ç”¨æˆ·ä¸“å±çš„ChromaDBè·¯å¾„
    
    Args:
        user_id: ç”¨æˆ·ID
        
    Returns:
        str: ç”¨æˆ·ä¸“å±çš„ChromaDBè·¯å¾„
    """
    user_path = os.path.join(CHROMA_BASE_PATH, f"user_{user_id}")
    os.makedirs(user_path, exist_ok=True)
    return user_path

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„CHROMA_PATHå˜é‡ï¼ˆä½†å»ºè®®ä½¿ç”¨get_user_chroma_pathï¼‰
CHROMA_PATH = CHROMA_BASE_PATH

# ========================= å…¨å±€çŠ¶æ€ç®¡ç† =========================

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªæ´»è·ƒçš„å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆæŒ‰ç”¨æˆ·åˆ†ç»„ï¼‰
# æ•°æ®ç»“æ„: {user_id: [vector_store_instances]}
# ç”¨äºå®ç°èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œå†…å­˜ä¼˜åŒ–
_active_vector_stores_by_user = {}

# ========================= æ—¥å¿—é…ç½® =========================

# é…ç½®æ—¥å¿—ç³»ç»Ÿï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ========================= æ–‡æ¡£åŠ è½½å¤„ç† =========================

def process_uploaded_file(file_path: str):
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆTXTã€PDFã€DOCXï¼‰
    - æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼Œä¼˜å…ˆä¸­æ–‡ç¼–ç 
    - ç»Ÿä¸€çš„æ–‡æ¡£å¯¹è±¡è¾“å‡ºæ ¼å¼
    - å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶
    
    Args:
        file_path (str): ä¸Šä¼ æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        
    Returns:
        list: LangChainæ–‡æ¡£å¯¹è±¡åˆ—è¡¨
        
    Raises:
        ValueError: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æˆ–ç¼–ç è§£æå¤±è´¥
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        
    Note:
        æ–‡æœ¬æ–‡ä»¶æ”¯æŒå¤šç§ç¼–ç è‡ªåŠ¨æ£€æµ‹ï¼šutf-8, gbk, gb2312, utf-8-sig, latin-1
    """
    try:
        if file_path.endswith('.txt'):
            # å°è¯•å¤šç§ç¼–ç æ–¹å¼åŠ è½½æ–‡æœ¬æ–‡ä»¶
            # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡ç¼–ç ï¼Œç¡®ä¿ä¸­æ–‡æ–‡æ¡£æ­£å¸¸åŠ è½½
            loader = None
            encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin-1']
            
            for encoding in encodings:
                try:
                    loader = TextLoader(file_path, encoding=encoding)
                    documents = loader.load()
                    logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æ–‡æœ¬æ–‡ä»¶: {file_path}")
                    return documents
                except UnicodeDecodeError:
                    logger.warning(f"ä½¿ç”¨ {encoding} ç¼–ç åŠ è½½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç ")
                    continue
                except Exception as e:
                    logger.warning(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç æ–¹å¼åŠ è½½æ–‡æœ¬æ–‡ä»¶: {file_path}")
            
        elif file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.docx'):
            try:
                loader = Docx2txtLoader(file_path)
            except Exception as docx_error:
                logger.error(f"åŠ è½½ .docx æ–‡ä»¶å¤±è´¥: {docx_error}")
                raise ValueError(f"æ— æ³•è¯»å– .docx æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶æœªæŸåä¸”æ ¼å¼æ­£ç¡®ã€‚é”™è¯¯ä¿¡æ¯: {str(docx_error)}")
        elif file_path.endswith('.doc'):
            # å¯¹äºæ—§ç‰ˆ .doc æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨ Docx2txtLoaderï¼ˆæœ‰æ—¶ä¹Ÿèƒ½å·¥ä½œï¼‰
            try:
                loader = Docx2txtLoader(file_path)
                logger.info("å°è¯•ä½¿ç”¨ Docx2txtLoader å¤„ç† .doc æ–‡ä»¶")
            except Exception as doc_error:
                logger.error(f"æ— æ³•å¤„ç† .doc æ–‡ä»¶: {doc_error}")
                raise ValueError(
                    "æ— æ³•å¤„ç†æ—§ç‰ˆ .doc æ–‡ä»¶æ ¼å¼ã€‚å»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n"
                    "1. å°†æ–‡ä»¶å¦å­˜ä¸º .docx æ ¼å¼åé‡æ–°ä¸Šä¼ \n"
                    "2. å°†æ–‡ä»¶å¦å­˜ä¸º .txt æ ¼å¼åä¸Šä¼ \n"
                    "3. ä½¿ç”¨ Microsoft Word æˆ– LibreOffice æ‰“å¼€æ–‡ä»¶å¹¶ä¿å­˜ä¸ºæ–°æ ¼å¼"
                )
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„æ ¼å¼ï¼štxtã€pdfã€docxã€‚å¯¹äº .doc æ–‡ä»¶ï¼Œè¯·å…ˆè½¬æ¢ä¸º .docx æ ¼å¼ã€‚")
        
        documents = loader.load()
        logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}, é¡µæ•°: {len(documents)}")
        return documents
    except Exception as e:
        logger.error(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
        raise

def split_documents(documents):
    """åˆ†å‰²æ–‡æ¡£ä¸ºé€‚åˆå¤„ç†çš„å°å—"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        length_function=len,
        is_separator_regex=False
    )
    
    return text_splitter.split_documents(documents)

def init_embeddings():
    """åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨ Ollama åµŒå…¥
        try:
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            # æµ‹è¯•åµŒå…¥æ˜¯å¦å·¥ä½œ
            test_embedding = embeddings.embed_query("test")
            if test_embedding and len(test_embedding) > 0:
                logger.info("ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹")
                return embeddings
        except Exception as ollama_error:
            logger.warning(f"Ollama åµŒå…¥ä¸å¯ç”¨: {ollama_error}")
        
        
    except Exception as e:
        logger.error(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise


def cleanup_vector_stores():
    """æ¸…ç†æ‰€æœ‰æ´»è·ƒçš„å‘é‡å­˜å‚¨è¿æ¥ï¼ˆå‘åå…¼å®¹ï¼‰"""
    global _active_vector_stores_by_user
    try:
        for user_id in list(_active_vector_stores_by_user.keys()):
            cleanup_user_vector_stores(user_id)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©ç³»ç»Ÿé‡Šæ”¾æ–‡ä»¶å¥æŸ„
        time.sleep(0.5)
        
    except Exception as e:
        logger.warning(f"æ¸…ç†å‘é‡å­˜å‚¨è¿æ¥å¤±è´¥: {e}")

def cleanup_user_vector_stores(user_id: str):
    """
    æ¸…ç†æŒ‡å®šç”¨æˆ·çš„æ´»è·ƒå‘é‡å­˜å‚¨è¿æ¥
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    global _active_vector_stores_by_user
    try:
        if user_id in _active_vector_stores_by_user:
            for vector_store in _active_vector_stores_by_user[user_id]:
                try:
                    # å°è¯•å…³é—­å‘é‡å­˜å‚¨è¿æ¥
                    if hasattr(vector_store, '_client') and vector_store._client:
                        vector_store._client.reset()
                    if hasattr(vector_store, '_collection'):
                        vector_store._collection = None
                    logger.info(f"ç”¨æˆ· {user_id} - å·²å…³é—­å‘é‡å­˜å‚¨è¿æ¥")
                except Exception as e:
                    logger.warning(f"ç”¨æˆ· {user_id} - å…³é—­å‘é‡å­˜å‚¨è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            _active_vector_stores_by_user[user_id].clear()
            
            # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œåˆ é™¤è¯¥ç”¨æˆ·çš„æ¡ç›®
            if not _active_vector_stores_by_user[user_id]:
                del _active_vector_stores_by_user[user_id]
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©ç³»ç»Ÿé‡Šæ”¾æ–‡ä»¶å¥æŸ„
        time.sleep(0.5)
        
    except Exception as e:
        logger.warning(f"ç”¨æˆ· {user_id} - æ¸…ç†å‘é‡å­˜å‚¨è¿æ¥å¤±è´¥: {e}")

def init_vector_store(documents, user_id: str = "default"):
    """
    åˆå§‹åŒ–ç”¨æˆ·ä¸“å±çš„å‘é‡å­˜å‚¨
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        user_id: ç”¨æˆ·IDï¼Œç”¨äºåˆ›å»ºç‹¬ç«‹çš„å­˜å‚¨ç©ºé—´
    """
    try:
        # å…ˆæ¸…é™¤è¯¥ç”¨æˆ·çš„ç°æœ‰å‘é‡å­˜å‚¨è¿æ¥
        cleanup_user_vector_stores(user_id)
        
        # è¿‡æ»¤ç©ºæ–‡æ¡£
        valid_docs = [doc for doc in documents if doc.page_content.strip()]
        if not valid_docs:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹å¯ä¾›å¤„ç†")
            
        logger.info(f"ç”¨æˆ· {user_id} - æœ‰æ•ˆæ–‡æ¡£æ•°é‡: {len(valid_docs)}/{len(documents)}")
        
        # è·å–ç”¨æˆ·ä¸“å±çš„ Chroma è·¯å¾„
        user_chroma_path = get_user_chroma_path(user_id)
        logger.info(f"ç”¨æˆ· {user_id} - ä½¿ç”¨å‘é‡æ•°æ®åº“è·¯å¾„: {user_chroma_path}")
        
        embeddings = init_embeddings()
        logger.info(f"ç”¨æˆ· {user_id} - åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        vector_store = Chroma.from_documents(
            documents=valid_docs,
            embedding=embeddings,
            persist_directory=user_chroma_path
        )
        
        # è·Ÿè¸ªæ´»è·ƒçš„å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆæŒ‰ç”¨æˆ·åˆ†ç»„ï¼‰
        if user_id not in _active_vector_stores_by_user:
            _active_vector_stores_by_user[user_id] = []
        _active_vector_stores_by_user[user_id].append(vector_store)
        
        # éªŒè¯å‘é‡å­˜å‚¨
        try:
            count = vector_store._collection.count()
            if count == 0:
                raise ValueError("å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥ï¼Œæœªæ’å…¥ä»»ä½•æ–‡æ¡£")
            logger.info(f"å‘é‡å­˜å‚¨éªŒè¯æˆåŠŸï¼Œæ–‡æ¡£æ•°é‡: {count}")
        except Exception as verification_error:
            logger.warning(f"å‘é‡å­˜å‚¨éªŒè¯å¤±è´¥: {verification_error}ï¼Œä½†ç»§ç»­å¤„ç†")
            
        logger.info(f"æˆåŠŸåˆå§‹åŒ–å‘é‡å­˜å‚¨, æ–‡æ¡£å—æ•°: {len(valid_docs)}")
        return vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )
    except Exception as e:
        logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise

def clear_vector_store():
    """æ¸…é™¤å‘é‡å­˜å‚¨ï¼ˆå‘åå…¼å®¹ï¼Œæ¸…é™¤æ‰€æœ‰ç”¨æˆ·æ•°æ®ï¼‰"""
    clear_all_user_documents()

def clear_user_vector_store(user_id: str):
    """
    æ¸…é™¤æŒ‡å®šç”¨æˆ·çš„å‘é‡å­˜å‚¨
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    try:
        # é¦–å…ˆæ¸…ç†è¯¥ç”¨æˆ·çš„æ´»è·ƒå‘é‡å­˜å‚¨è¿æ¥
        cleanup_user_vector_stores(user_id)
        
        user_chroma_path = get_user_chroma_path(user_id)
        
        if os.path.exists(user_chroma_path):
            # å°è¯•å¤šæ¬¡åˆ é™¤ï¼Œå¦‚æœæ–‡ä»¶è¢«å ç”¨åˆ™ç­‰å¾…
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    shutil.rmtree(user_chroma_path)
                    logger.info(f"ç”¨æˆ· {user_id} - å·²æ¸…é™¤å‘é‡å­˜å‚¨: {user_chroma_path}")
                    break
                except PermissionError as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"ç”¨æˆ· {user_id} - æ–‡ä»¶è¢«å ç”¨ï¼Œç­‰å¾…åé‡è¯• (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        time.sleep(1)
                        continue
                    else:
                        # å¦‚æœä»ç„¶æ— æ³•åˆ é™¤ï¼Œå°è¯•æ¸…ç©ºç›®å½•å†…å®¹
                        logger.warning(f"ç”¨æˆ· {user_id} - æ— æ³•åˆ é™¤æ•´ä¸ªç›®å½•ï¼Œå°è¯•æ¸…ç©ºå†…å®¹: {e}")
                        try:
                            for root, dirs, files in os.walk(user_chroma_path, topdown=False):
                                for file in files:
                                    try:
                                        os.remove(os.path.join(root, file))
                                    except:
                                        pass
                                for dir in dirs:
                                    try:
                                        os.rmdir(os.path.join(root, dir))
                                    except:
                                        pass
                            logger.info(f"ç”¨æˆ· {user_id} - å·²æ¸…ç©ºå‘é‡å­˜å‚¨ç›®å½•å†…å®¹")
                        except Exception as cleanup_error:
                            logger.warning(f"ç”¨æˆ· {user_id} - æ¸…ç©ºç›®å½•å†…å®¹ä¹Ÿå¤±è´¥: {cleanup_error}")
        else:
            logger.info(f"ç”¨æˆ· {user_id} - å‘é‡å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {user_chroma_path}")
    except Exception as e:
        logger.error(f"ç”¨æˆ· {user_id} - æ¸…é™¤å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
        raise

def clear_all_user_documents():
    """æ¸…é™¤æ‰€æœ‰ç”¨æˆ·çš„æ–‡æ¡£æ•°æ®"""
    try:
        # é¦–å…ˆæ¸…ç†æ‰€æœ‰æ´»è·ƒçš„å‘é‡å­˜å‚¨è¿æ¥
        cleanup_vector_stores()
        
        if os.path.exists(CHROMA_BASE_PATH):
            # å°è¯•å¤šæ¬¡åˆ é™¤ï¼Œå¦‚æœæ–‡ä»¶è¢«å ç”¨åˆ™ç­‰å¾…
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    shutil.rmtree(CHROMA_BASE_PATH)
                    logger.info(f"å·²æ¸…é™¤æ‰€æœ‰ç”¨æˆ·å‘é‡å­˜å‚¨: {CHROMA_BASE_PATH}")
                    # é‡æ–°åˆ›å»ºåŸºç¡€ç›®å½•
                    os.makedirs(CHROMA_BASE_PATH, exist_ok=True)
                    break
                except PermissionError as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"æ–‡ä»¶è¢«å ç”¨ï¼Œç­‰å¾…åé‡è¯• (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        time.sleep(1)
                        continue
                    else:
                        # å¦‚æœä»ç„¶æ— æ³•åˆ é™¤ï¼Œå°è¯•æ¸…ç©ºç›®å½•å†…å®¹
                        logger.warning(f"æ— æ³•åˆ é™¤æ•´ä¸ªç›®å½•ï¼Œå°è¯•æ¸…ç©ºå†…å®¹: {e}")
                        try:
                            for root, dirs, files in os.walk(CHROMA_BASE_PATH, topdown=False):
                                for file in files:
                                    try:
                                        os.remove(os.path.join(root, file))
                                    except:
                                        pass
                                for dir in dirs:
                                    try:
                                        os.rmdir(os.path.join(root, dir))
                                    except:
                                        pass
                            logger.info("å·²æ¸…ç©ºæ‰€æœ‰ç”¨æˆ·å‘é‡å­˜å‚¨ç›®å½•å†…å®¹")
                        except Exception as cleanup_error:
                            logger.warning(f"æ¸…ç©ºç›®å½•å†…å®¹ä¹Ÿå¤±è´¥: {cleanup_error}")
        else:
            logger.info(f"å‘é‡å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {CHROMA_BASE_PATH}")
                            
        os.makedirs(CHROMA_PATH, exist_ok=True)
        logger.info("å‘é‡å­˜å‚¨å·²é‡ç½®")
    except Exception as e:
        logger.error(f"æ¸…é™¤å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…é˜»å¡å…¶ä»–æ¸…ç†æ“ä½œ

def clear_uploaded_files():
    """æ¸…é™¤æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        from config import UPLOAD_DIR
        if os.path.exists(UPLOAD_DIR):
            # åˆ é™¤ä¸Šä¼ ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for filename in os.listdir(UPLOAD_DIR):
                file_path = os.path.join(UPLOAD_DIR, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logger.info(f"å·²åˆ é™¤ä¸Šä¼ æ–‡ä»¶: {filename}")
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                        logger.info(f"å·²åˆ é™¤ä¸Šä¼ ç›®å½•: {filename}")
                except Exception as file_error:
                    logger.warning(f"åˆ é™¤æ–‡ä»¶ {filename} å¤±è´¥: {file_error}")
            logger.info("æ‰€æœ‰ä¸Šä¼ æ–‡ä»¶å·²æ¸…é™¤")
        else:
            logger.info("ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…é™¤")
    except Exception as e:
        logger.error(f"æ¸…é™¤ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise

def clear_all_document_data():
    """æ¸…é™¤æ‰€æœ‰æ–‡æ¡£ç›¸å…³æ•°æ®ï¼ˆå‘é‡å­˜å‚¨å’Œä¸Šä¼ æ–‡ä»¶ï¼‰- å‘åå…¼å®¹"""
    clear_all_user_documents()

def clear_user_document_data(user_id: str):
    """
    æ¸…é™¤æŒ‡å®šç”¨æˆ·çš„æ–‡æ¡£ç›¸å…³æ•°æ®ï¼ˆå‘é‡å­˜å‚¨å’Œä¸Šä¼ æ–‡ä»¶ï¼‰
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    errors = []
    
    # åˆ†åˆ«å°è¯•æ¸…é™¤å‘é‡å­˜å‚¨å’Œä¸Šä¼ æ–‡ä»¶ï¼Œå³ä½¿å…¶ä¸­ä¸€ä¸ªå¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œå¦ä¸€ä¸ª
    try:
        clear_user_vector_store(user_id)
        logger.info(f"ç”¨æˆ· {user_id} - å‘é‡å­˜å‚¨æ¸…é™¤å®Œæˆ")
    except Exception as e:
        error_msg = f"ç”¨æˆ· {user_id} - æ¸…é™¤å‘é‡å­˜å‚¨å¤±è´¥: {e}"
        logger.error(error_msg)
        errors.append(error_msg)
    
    try:
        clear_user_uploaded_files(user_id)
        logger.info(f"ç”¨æˆ· {user_id} - ä¸Šä¼ æ–‡ä»¶æ¸…é™¤å®Œæˆ")
    except Exception as e:
        error_msg = f"ç”¨æˆ· {user_id} - æ¸…é™¤ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}"
        logger.error(error_msg)
        errors.append(error_msg)
    
    if errors:
        # å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•ä½†ä¸æŠ›å‡ºå¼‚å¸¸
        logger.warning(f"ç”¨æˆ· {user_id} - æ¸…é™¤æ–‡æ¡£æ•°æ®æ—¶é‡åˆ°éƒ¨åˆ†é”™è¯¯: {'; '.join(errors)}")
    else:
        logger.info(f"ç”¨æˆ· {user_id} - æ‰€æœ‰æ–‡æ¡£æ•°æ®æ¸…é™¤æˆåŠŸ")

def clear_user_uploaded_files(user_id: str):
    """
    æ¸…é™¤æŒ‡å®šç”¨æˆ·çš„ä¸Šä¼ æ–‡ä»¶
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    from config import UPLOAD_DIR
    
    try:
        user_upload_dir = os.path.join(UPLOAD_DIR, f"user_{user_id}")
        if os.path.exists(user_upload_dir):
            shutil.rmtree(user_upload_dir)
            logger.info(f"ç”¨æˆ· {user_id} - å·²æ¸…é™¤ä¸Šä¼ æ–‡ä»¶: {user_upload_dir}")
        else:
            logger.info(f"ç”¨æˆ· {user_id} - ä¸Šä¼ æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {user_upload_dir}")
    except Exception as e:
        logger.error(f"ç”¨æˆ· {user_id} - æ¸…é™¤ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise
        logger.warning(f"æ–‡æ¡£æ•°æ®æ¸…é™¤è¿‡ç¨‹ä¸­å‡ºç°ä»¥ä¸‹é—®é¢˜: {'; '.join(errors)}")
    else:
        logger.info("æ‰€æœ‰æ–‡æ¡£æ•°æ®å·²æ¸…é™¤")

def generate_document_summary(documents, max_length=500):
    """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
    try:
        content = " ".join([doc.page_content for doc in documents[:3]])
        if len(content) > max_length:
            return content[:max_length] + "..."
        return content
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦"