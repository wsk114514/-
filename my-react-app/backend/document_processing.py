"""
document_processing.py - æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–æ¨¡å—

è¿™æ˜¯RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
1. ğŸ“„ å¤šæ ¼å¼æ–‡æ¡£åŠ è½½ - æ”¯æŒTXTã€PDFã€DOCXæ–‡ä»¶
2. ğŸ”¤ æ™ºèƒ½æ–‡æœ¬åˆ†å‰² - é€’å½’å­—ç¬¦åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
3. ğŸ§® å‘é‡åŒ–å­˜å‚¨ - ä½¿ç”¨ChromaDBæ„å»ºå‘é‡æ•°æ®åº“
4. ğŸ” ç›¸ä¼¼æ€§æ£€ç´¢ - åŸºäºåµŒå…¥å‘é‡çš„æ–‡æ¡£æ£€ç´¢
5. ğŸ—‘ï¸ èµ„æºç®¡ç† - è‡ªåŠ¨å†…å­˜ç®¡ç†å’Œå®ä¾‹æ¸…ç†
6. ğŸŒ å¤šç¼–ç æ”¯æŒ - æ™ºèƒ½æ£€æµ‹å’Œå¤„ç†å¤šç§æ–‡æœ¬ç¼–ç 

æŠ€æœ¯æ ˆ:
- LangChain: æ–‡æ¡£åŠ è½½å’Œå¤„ç†æ¡†æ¶
- ChromaDB: å‘é‡æ•°æ®åº“
- Ollama Embeddings: æœ¬åœ°åµŒå…¥æ¨¡å‹
- RecursiveCharacterTextSplitter: æ™ºèƒ½æ–‡æœ¬åˆ†å‰²

è®¾è®¡ç‰¹è‰²:
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„ç»Ÿä¸€å¤„ç†
- æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼Œå…¼å®¹ä¸­æ–‡ç¯å¢ƒ
- èµ„æºç”Ÿå‘½å‘¨æœŸè‡ªåŠ¨ç®¡ç†
- é”™è¯¯æ¢å¤å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶
"""

import os
import shutil
import time
import gc
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings  # æ›´æ–°åçš„å¯¼å…¥æ–¹å¼
import logging

# ========================= ç¯å¢ƒé…ç½® =========================

# ç¦ç”¨ ChromaDB çš„é¥æµ‹åŠŸèƒ½ï¼Œä¿æŠ¤ç”¨æˆ·éšç§
os.environ["ANONYMIZED_TELEMETRY"] = "False"

# è®¾ç½®Chromaæ•°æ®åº“è·¯å¾„
CHROMA_PATH = "chroma_db"
# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(CHROMA_PATH, exist_ok=True)

# ========================= å…¨å±€çŠ¶æ€ç®¡ç† =========================

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªæ´»è·ƒçš„å‘é‡å­˜å‚¨å®ä¾‹
# ç”¨äºå®ç°èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œå†…å­˜ä¼˜åŒ–
_active_vector_stores = []

# ========================= æ—¥å¿—é…ç½® =========================

# é…ç½®æ—¥å¿—ç³»ç»Ÿï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ========================= æ–‡æ¡£åŠ è½½å¤„ç† =========================

def process_uploaded_file(file_path: str):
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£
    
    åŠŸèƒ½è¯´æ˜ï¼š
    - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆTXTã€PDFã€DOCXï¼‰
    - æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼Œä¼˜å…ˆä¸­æ–‡ç¼–ç 
    - ç»Ÿä¸€çš„æ–‡æ¡£å¯¹è±¡è¾“å‡ºæ ¼å¼
    - å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶
    
    Args:
        file_path (str): ä¸Šä¼ æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        
    Returns:
        list: LangChainæ–‡æ¡£å¯¹è±¡åˆ—è¡¨
        
    Raises:
        ValueError: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æˆ–ç¼–ç è§£æå¤±è´¥
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        
    Note:
        æ–‡æœ¬æ–‡ä»¶æ”¯æŒå¤šç§ç¼–ç è‡ªåŠ¨æ£€æµ‹ï¼šutf-8, gbk, gb2312, utf-8-sig, latin-1
    """
    try:
        if file_path.endswith('.txt'):
            # å°è¯•å¤šç§ç¼–ç æ–¹å¼åŠ è½½æ–‡æœ¬æ–‡ä»¶
            # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡ç¼–ç ï¼Œç¡®ä¿ä¸­æ–‡æ–‡æ¡£æ­£å¸¸åŠ è½½
            loader = None
            encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin-1']
            
            for encoding in encodings:
                try:
                    loader = TextLoader(file_path, encoding=encoding)
                    documents = loader.load()
                    logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æ–‡æœ¬æ–‡ä»¶: {file_path}")
                    return documents
                except UnicodeDecodeError:
                    logger.warning(f"ä½¿ç”¨ {encoding} ç¼–ç åŠ è½½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç ")
                    continue
                except Exception as e:
                    logger.warning(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç æ–¹å¼åŠ è½½æ–‡æœ¬æ–‡ä»¶: {file_path}")
            
        elif file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.docx'):
            try:
                loader = Docx2txtLoader(file_path)
            except Exception as docx_error:
                logger.error(f"åŠ è½½ .docx æ–‡ä»¶å¤±è´¥: {docx_error}")
                raise ValueError(f"æ— æ³•è¯»å– .docx æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶æœªæŸåä¸”æ ¼å¼æ­£ç¡®ã€‚é”™è¯¯ä¿¡æ¯: {str(docx_error)}")
        elif file_path.endswith('.doc'):
            # å¯¹äºæ—§ç‰ˆ .doc æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨ Docx2txtLoaderï¼ˆæœ‰æ—¶ä¹Ÿèƒ½å·¥ä½œï¼‰
            try:
                loader = Docx2txtLoader(file_path)
                logger.info("å°è¯•ä½¿ç”¨ Docx2txtLoader å¤„ç† .doc æ–‡ä»¶")
            except Exception as doc_error:
                logger.error(f"æ— æ³•å¤„ç† .doc æ–‡ä»¶: {doc_error}")
                raise ValueError(
                    "æ— æ³•å¤„ç†æ—§ç‰ˆ .doc æ–‡ä»¶æ ¼å¼ã€‚å»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n"
                    "1. å°†æ–‡ä»¶å¦å­˜ä¸º .docx æ ¼å¼åé‡æ–°ä¸Šä¼ \n"
                    "2. å°†æ–‡ä»¶å¦å­˜ä¸º .txt æ ¼å¼åä¸Šä¼ \n"
                    "3. ä½¿ç”¨ Microsoft Word æˆ– LibreOffice æ‰“å¼€æ–‡ä»¶å¹¶ä¿å­˜ä¸ºæ–°æ ¼å¼"
                )
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„æ ¼å¼ï¼štxtã€pdfã€docxã€‚å¯¹äº .doc æ–‡ä»¶ï¼Œè¯·å…ˆè½¬æ¢ä¸º .docx æ ¼å¼ã€‚")
        
        documents = loader.load()
        logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}, é¡µæ•°: {len(documents)}")
        return documents
    except Exception as e:
        logger.error(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
        raise

def split_documents(documents):
    """åˆ†å‰²æ–‡æ¡£ä¸ºé€‚åˆå¤„ç†çš„å°å—"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        length_function=len,
        is_separator_regex=False
    )
    
    return text_splitter.split_documents(documents)

def init_embeddings():
    """åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨ Ollama åµŒå…¥
        try:
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            # æµ‹è¯•åµŒå…¥æ˜¯å¦å·¥ä½œ
            test_embedding = embeddings.embed_query("test")
            if test_embedding and len(test_embedding) > 0:
                logger.info("ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹")
                return embeddings
        except Exception as ollama_error:
            logger.warning(f"Ollama åµŒå…¥ä¸å¯ç”¨: {ollama_error}")
        
        
    except Exception as e:
        logger.error(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise


def cleanup_vector_stores():
    """æ¸…ç†æ‰€æœ‰æ´»è·ƒçš„å‘é‡å­˜å‚¨è¿æ¥"""
    global _active_vector_stores
    try:
        for vector_store in _active_vector_stores:
            try:
                # å°è¯•å…³é—­å‘é‡å­˜å‚¨è¿æ¥
                if hasattr(vector_store, '_client') and vector_store._client:
                    vector_store._client.reset()
                if hasattr(vector_store, '_collection'):
                    vector_store._collection = None
                logger.info("å·²å…³é—­å‘é‡å­˜å‚¨è¿æ¥")
            except Exception as e:
                logger.warning(f"å…³é—­å‘é‡å­˜å‚¨è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        _active_vector_stores.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©ç³»ç»Ÿé‡Šæ”¾æ–‡ä»¶å¥æŸ„
        time.sleep(0.5)
        
    except Exception as e:
        logger.warning(f"æ¸…ç†å‘é‡å­˜å‚¨è¿æ¥å¤±è´¥: {e}")

def init_vector_store(documents):
    """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
    try:
        # å…ˆæ¸…é™¤ä»»ä½•ç°æœ‰çš„å‘é‡å­˜å‚¨è¿æ¥
        cleanup_vector_stores()
        
        # è¿‡æ»¤ç©ºæ–‡æ¡£
        valid_docs = [doc for doc in documents if doc.page_content.strip()]
        if not valid_docs:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹å¯ä¾›å¤„ç†")
            
        logger.info(f"æœ‰æ•ˆæ–‡æ¡£æ•°é‡: {len(valid_docs)}/{len(documents)}")
        
        # ç¡®ä¿ Chroma ç›®å½•å­˜åœ¨
        os.makedirs(CHROMA_PATH, exist_ok=True)
        
        embeddings = init_embeddings()
        logger.info("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        vector_store = Chroma.from_documents(
            documents=valid_docs,
            embedding=embeddings,
            persist_directory=CHROMA_PATH
        )
        
        # è·Ÿè¸ªæ´»è·ƒçš„å‘é‡å­˜å‚¨å®ä¾‹
        _active_vector_stores.append(vector_store)
        
        # éªŒè¯å‘é‡å­˜å‚¨
        try:
            count = vector_store._collection.count()
            if count == 0:
                raise ValueError("å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥ï¼Œæœªæ’å…¥ä»»ä½•æ–‡æ¡£")
            logger.info(f"å‘é‡å­˜å‚¨éªŒè¯æˆåŠŸï¼Œæ–‡æ¡£æ•°é‡: {count}")
        except Exception as verification_error:
            logger.warning(f"å‘é‡å­˜å‚¨éªŒè¯å¤±è´¥: {verification_error}ï¼Œä½†ç»§ç»­å¤„ç†")
            
        logger.info(f"æˆåŠŸåˆå§‹åŒ–å‘é‡å­˜å‚¨, æ–‡æ¡£å—æ•°: {len(valid_docs)}")
        return vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )
    except Exception as e:
        logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise

def clear_vector_store():
    """æ¸…é™¤å‘é‡å­˜å‚¨"""
    try:
        # é¦–å…ˆæ¸…ç†æ‰€æœ‰æ´»è·ƒçš„å‘é‡å­˜å‚¨è¿æ¥
        cleanup_vector_stores()
        
        if os.path.exists(CHROMA_PATH):
            # å°è¯•å¤šæ¬¡åˆ é™¤ï¼Œå¦‚æœæ–‡ä»¶è¢«å ç”¨åˆ™ç­‰å¾…
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    shutil.rmtree(CHROMA_PATH)
                    logger.info(f"å·²æ¸…é™¤å‘é‡å­˜å‚¨: {CHROMA_PATH}")
                    break
                except PermissionError as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"æ–‡ä»¶è¢«å ç”¨ï¼Œç­‰å¾…åé‡è¯• (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        time.sleep(1)
                        continue
                    else:
                        # å¦‚æœä»ç„¶æ— æ³•åˆ é™¤ï¼Œå°è¯•æ¸…ç©ºç›®å½•å†…å®¹
                        logger.warning(f"æ— æ³•åˆ é™¤æ•´ä¸ªç›®å½•ï¼Œå°è¯•æ¸…ç©ºå†…å®¹: {e}")
                        try:
                            for root, dirs, files in os.walk(CHROMA_PATH, topdown=False):
                                for file in files:
                                    try:
                                        os.remove(os.path.join(root, file))
                                    except:
                                        pass
                                for dir in dirs:
                                    try:
                                        os.rmdir(os.path.join(root, dir))
                                    except:
                                        pass
                            logger.info("å·²æ¸…ç©ºå‘é‡å­˜å‚¨ç›®å½•å†…å®¹")
                        except Exception as cleanup_error:
                            logger.warning(f"æ¸…ç©ºç›®å½•å†…å®¹ä¹Ÿå¤±è´¥: {cleanup_error}")
                            # è®°å½•é”™è¯¯ä½†ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…é˜»å¡å…¶ä»–æ“ä½œ
                            
        os.makedirs(CHROMA_PATH, exist_ok=True)
        logger.info("å‘é‡å­˜å‚¨å·²é‡ç½®")
    except Exception as e:
        logger.error(f"æ¸…é™¤å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…é˜»å¡å…¶ä»–æ¸…ç†æ“ä½œ

def clear_uploaded_files():
    """æ¸…é™¤æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        from config import UPLOAD_DIR
        if os.path.exists(UPLOAD_DIR):
            # åˆ é™¤ä¸Šä¼ ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for filename in os.listdir(UPLOAD_DIR):
                file_path = os.path.join(UPLOAD_DIR, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logger.info(f"å·²åˆ é™¤ä¸Šä¼ æ–‡ä»¶: {filename}")
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                        logger.info(f"å·²åˆ é™¤ä¸Šä¼ ç›®å½•: {filename}")
                except Exception as file_error:
                    logger.warning(f"åˆ é™¤æ–‡ä»¶ {filename} å¤±è´¥: {file_error}")
            logger.info("æ‰€æœ‰ä¸Šä¼ æ–‡ä»¶å·²æ¸…é™¤")
        else:
            logger.info("ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…é™¤")
    except Exception as e:
        logger.error(f"æ¸…é™¤ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise

def clear_all_document_data():
    """æ¸…é™¤æ‰€æœ‰æ–‡æ¡£ç›¸å…³æ•°æ®ï¼ˆå‘é‡å­˜å‚¨å’Œä¸Šä¼ æ–‡ä»¶ï¼‰"""
    errors = []
    
    # åˆ†åˆ«å°è¯•æ¸…é™¤å‘é‡å­˜å‚¨å’Œä¸Šä¼ æ–‡ä»¶ï¼Œå³ä½¿å…¶ä¸­ä¸€ä¸ªå¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œå¦ä¸€ä¸ª
    try:
        clear_vector_store()
        logger.info("å‘é‡å­˜å‚¨æ¸…é™¤å®Œæˆ")
    except Exception as e:
        error_msg = f"æ¸…é™¤å‘é‡å­˜å‚¨å¤±è´¥: {e}"
        logger.error(error_msg)
        errors.append(error_msg)
    
    try:
        clear_uploaded_files()
        logger.info("ä¸Šä¼ æ–‡ä»¶æ¸…é™¤å®Œæˆ")
    except Exception as e:
        error_msg = f"æ¸…é™¤ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}"
        logger.error(error_msg)
        errors.append(error_msg)
    
    if errors:
        # å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•ä½†ä¸æŠ›å‡ºå¼‚å¸¸
        logger.warning(f"æ–‡æ¡£æ•°æ®æ¸…é™¤è¿‡ç¨‹ä¸­å‡ºç°ä»¥ä¸‹é—®é¢˜: {'; '.join(errors)}")
    else:
        logger.info("æ‰€æœ‰æ–‡æ¡£æ•°æ®å·²æ¸…é™¤")

def generate_document_summary(documents, max_length=500):
    """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
    try:
        content = " ".join([doc.page_content for doc in documents[:3]])
        if len(content) > max_length:
            return content[:max_length] + "..."
        return content
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
        return "æ— æ³•ç”Ÿæˆæ‘˜è¦"